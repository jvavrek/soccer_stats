{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_df = pd.DataFrame.from_csv(\"./feature_matrix.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_df = data_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date', 'Year', 'Type', 'Group', 'Country1', 'Score', 'Country2',\n",
       "       'Stadium', 'Ref', 'Attendence', 'Earth Level', 'Score1', 'Score2',\n",
       "       'Score_Diff', 'Matches Played_C1', 'Yellow_Per_Game_Avg_C1',\n",
       "       'YellowRed_Per_Game_Avg_C1', 'Red_Per_Game_Avg_C1',\n",
       "       'Goal_Per_Game_Avg_C1', 'Goal_Against_Per_Game_Avg_C1',\n",
       "       'PenGoal_Per_Game_Avg_C1', 'Matches Played_C2',\n",
       "       'Yellow_Per_Game_Avg_C2', 'YellowRed_Per_Game_Avg_C2',\n",
       "       'Red_Per_Game_Avg_C2', 'Goal_Per_Game_Avg_C2',\n",
       "       'Goal_Against_Per_Game_Avg_C2', 'PenGoal_Per_Game_Avg_C2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Make Output Predictor Win, Loss, or Tie\n",
    "data_df[\"Result\"] = False\n",
    "\n",
    "data_df.loc[data_df['Score_Diff'] > 0,\"Result\"] = True\n",
    "#data_df.loc[data_df['Score_Diff'] < 0,\"Result\"] = \"Loss\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Date',\n",
       " 'Year',\n",
       " 'Type',\n",
       " 'Group',\n",
       " 'Country1',\n",
       " 'Score',\n",
       " 'Country2',\n",
       " 'Stadium',\n",
       " 'Ref',\n",
       " 'Attendence',\n",
       " 'Earth Level',\n",
       " 'Score1',\n",
       " 'Score2',\n",
       " 'Score_Diff',\n",
       " 'Matches Played_C1',\n",
       " 'Yellow_Per_Game_Avg_C1',\n",
       " 'YellowRed_Per_Game_Avg_C1',\n",
       " 'Red_Per_Game_Avg_C1',\n",
       " 'Goal_Per_Game_Avg_C1',\n",
       " 'Goal_Against_Per_Game_Avg_C1',\n",
       " 'PenGoal_Per_Game_Avg_C1',\n",
       " 'Matches Played_C2',\n",
       " 'Yellow_Per_Game_Avg_C2',\n",
       " 'YellowRed_Per_Game_Avg_C2',\n",
       " 'Red_Per_Game_Avg_C2',\n",
       " 'Goal_Per_Game_Avg_C2',\n",
       " 'Goal_Against_Per_Game_Avg_C2',\n",
       " 'PenGoal_Per_Game_Avg_C2',\n",
       " 'Result']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_to_use = ['Earth Level',\n",
    "                 'Matches Played_C1',\n",
    "                 'Yellow_Per_Game_Avg_C1',\n",
    "                 'YellowRed_Per_Game_Avg_C1',\n",
    "                 'Red_Per_Game_Avg_C1',\n",
    "                 'Goal_Per_Game_Avg_C1',\n",
    "                 'Goal_Against_Per_Game_Avg_C1',\n",
    "                 'PenGoal_Per_Game_Avg_C1',\n",
    "                 'Matches Played_C2',\n",
    "                 'Yellow_Per_Game_Avg_C2',\n",
    "                 'YellowRed_Per_Game_Avg_C2',\n",
    "                 'Red_Per_Game_Avg_C2',\n",
    "                 'Goal_Per_Game_Avg_C2',\n",
    "                 'Goal_Against_Per_Game_Avg_C2',\n",
    "                 'PenGoal_Per_Game_Avg_C2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data_df[features_to_use].as_matrix()\n",
    "y = data_df[\"Result\"].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Sample Accuracy = 0.700336700337\n",
      "Out Sample Accuracy = 0.64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(max_depth=3, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "in_sample_pred = clf.predict(X_train)\n",
    "out_sample_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "Accuracy = (in_sample_pred == y_train).sum()/len(y_train)\n",
    "print(\"In Sample Accuracy = %s\"%Accuracy)\n",
    "\n",
    "Accuracy = (out_sample_pred == y_test).sum()/len(y_test)\n",
    "print(\"Out Sample Accuracy = %s\"%Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Sample Accuracy = 0.690235690236\n",
      "Out Sample Accuracy = 0.626666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "in_sample_pred = clf.predict(X_train)\n",
    "out_sample_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "Accuracy = (in_sample_pred == y_train).sum()/len(y_train)\n",
    "print(\"In Sample Accuracy = %s\"%Accuracy)\n",
    "\n",
    "Accuracy = (out_sample_pred == y_test).sum()/len(y_test)\n",
    "print(\"Out Sample Accuracy = %s\"%Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Sample Accuracy = 0.922558922559\n",
      "Out Sample Accuracy = 0.64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=1000, learning_rate=1, random_state=None)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "in_sample_pred = clf.predict(X_train)\n",
    "out_sample_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "Accuracy = (in_sample_pred == y_train).sum()/len(y_train)\n",
    "print(\"In Sample Accuracy = %s\"%Accuracy)\n",
    "\n",
    "Accuracy = (out_sample_pred == y_test).sum()/len(y_test)\n",
    "print(\"Out Sample Accuracy = %s\"%Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "279"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
